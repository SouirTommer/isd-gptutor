{
  "pdfs": [
    {
      "id": "1743501813913",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:03:33.913Z"
    },
    {
      "id": "1743502172827",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:09:32.827Z",
      "isMockData": true
    },
    {
      "id": "1743502393977",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:13:13.977Z",
      "isMockData": true
    },
    {
      "id": "1743502442118",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:14:02.118Z",
      "isMockData": true
    },
    {
      "id": "1743503110461",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:25:10.461Z",
      "isMockData": true
    },
    {
      "id": "1743503434646",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:30:34.646Z",
      "isMockData": true
    },
    {
      "id": "1743503838771",
      "fileName": "5-Software Testing.pdf",
      "originalText": "\n\nSoftware Testing\n1\n\nWhat is software testing?\n•Software testing is an investigation conducted to provide stakeholders with \ninformation about the quality of  the product or service under test\n2\nQuality\nMeet user's \nneeds\nConformance \nto \nrequirements\nCustomer's \nwillingness to \npay\nFree of errors \nand failure\nCustomer's \nsatisfaction\nFitness of use\n\nThe V Model\nDevelopment\nTesting\nAcceptance Testing\nUnit Testing\nIntegration Testing\nSystem Testing\nAcceptance test \ndesign\nSystem test \ndesign\nIntegration test \ndesign\nUnit test \ndesign\nUser's \nneeds\n3\nVerification: building product correctly\nValidation: building the correct product\nRequirements\nArchitecture Design\nCoding\nSystem Design\nModule Design\nTesting TypeDescriptionPurpose\nWho \nPerforms It\nValidation / \nVerification\nUnit Testing\nTesting \nindividual \ncomponents \nor functions.\nTo verify that \neach unit \nfunctions \ncorrectly.\nDevelopersVerification\nIntegration \nTesting\nTesting the \ninteraction \nbetween \nintegrated \nunits or \ncomponents.\nTo ensure \ncombined \nparts work \ntogether.\nDevelopers/Q\nA Engineers\nVerification\nSystem \nTesting\nTesting the \ncomplete and \nintegrated \nsoftware \nsystem.\nTo validate \nthe end-to-\nend system \nspecifications.\nQA EngineersValidation\nAcceptance \nTesting\nTesting to \ndetermine if \nthe system \nmeets \nbusiness \nrequirements.\nTo confirm \nreadiness for \ndelivery to \nusers.\nEnd users/\nStakeholders\nValidation\n\nConfirmation vs. Regression Testing\n4\n•Confirmation testing\n•Testing that run test cases that failed the last time, in order to verify the success of  the correctness \naction.\n•Regression Testing\n•Regression:  The misbehavior of  a previously correct function, attribute or feature\n•Testing of  a previously tested program following modification (e.g. after bug fixing or adding new \nfeatures) \n•Existing test cases are re-run to ensure that defects have not been introduced or uncovered in \nunchanged area of  the software as a result of  change.\n\nExample\n5\n•Receive a feature‐completetestrelease and start testing. \n•Testfeatures 1 and 2 without any problem.\n•Find a bug in feature in 3, which is fixed by the programmer.\n•Regression testing for feature 1 and 2\n•Major update, release 1.1\n•Test the new features\n•Re-test the features from the first release\n\nStrategies for regression testing\n6\n•Conservative approach: re-run all test cases\n•Test automation may speed up regression testing.\n•Select subset of  features for re-testing\n•E.g. Run only tests that reach or related to the modified statements/modules \n\nWhat are the test cases in WordPad?\n7\n\nWhat are the test cases in GPTutor?\n8\n•What can the user input in this textbox?\n•What is the expected output for the different types of input?\n•What are the challenges in testing an application based on LLM?\nExercise\nThink of the different types of test cases we may need to handle. \nFor each type of test case, define the expected response of the chatbot. Does it meet your expectation?\n\nComplete testing is impossible\n9\n•Testing everything (all combinations of  input and preconditions) is not feasible except in trivial \ncases.\n•The number of  possible inputs is very large.\n•The number of  possible outputs is very large.\n•The number of  paths through the software is very large.\n•The software specification is subjective. You might say that a bug is in the eye of  the beholder.\nInstead of running all the test cases, we select only the most \nrepresentative test cases to test the program!\n\nTest Coverage\n10\n\nWhite box vs. Black box testing\n11\nBlack-box testing \n•Aka specification testing\n•Test case generation are based solely \non the knowledge of  the system \nrequirements.\n•Ensures that specified features of  the \nsoftware are addressed by some \naspect of  the program\nWhite-box testing\n•Aka Code-based testing \n•Test data selection is guided by \ninformation derived from the internal \nstructure and internal functional \nproperties of  the program\n\nTest Coverage\n12\n•Helps answer the following questions\n•Is your software tested sufficiently?\n•In what software component you haven’t tested sufficiently?\n•Black box coverage criterion \n•Requirements coverage: # of  tested requirement/total requirements\n•White box coverage criterion\n•Statement coverage\n•Decision coverage\n•Condition coverage\n•Multiple condition coverage\n•Modified condition/Decision coverage (MC/DC) \n\nExample: Requirement Coverage\n1.Requirement: Minimum and Maximum Length\n1.The username field should accept a minimum of  6 characters and a maximum of  20 characters.\n2.The password field should accept a minimum of  8 characters and a maximum of  30 characters.\n2.Requirement: Character Types Accepted\n1.The username field should accept alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", and \"-\".\n2.The password field should accept a combination of  alphanumeric characters (A-Z, a-z, 0-9) and special characters such as \".\", \"_\", \"@\", and \n\"#\".\n3.Requirement: Character Types Not Accepted\n1.The username field should not accept spaces or any other special characters apart from \".\", \"_\", and \"-\".\n2.The password field should not accept spaces or any special characters other than \".\", \"_\", \"@\", and \"#\".\n4.Requirement: Password Strength\n1.The password field should enforce a minimum level of  complexity by requiring at least one uppercase letter, one lowercase letter, one \nnumeric digit, and one special character.\n13\nEach requirement should be covered by at least ONE test case.\n\nStatement coverage\n14\n•Test cases should cover all the executable statements \n•Excluding comments, empty lines, etc.\nexamScore= int(input(\"Enter exam score: \"))\nisPass= False\nifexamScore>= 70:\nisPass= True\nprint(isPass)\nStatement Coverage =\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1800.8True\n\nDecision (Branch) Coverage\n15\n•Derive test cases to cover all the decisions (branches) in the program. \n•A decision/branch is covered if  it evaluates to both true and false outcome by at \nleast one test cases.\nTwo decisions outcomes to be covered:\nexamScore: {T,F}\nexamScore= int(input(\"Enter exam score: \"))\nifexamScore>= 70:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreExpected Result\n1\n2\nDecision Coverage =\n\nCondition Coverage\n16\n•A decision may be composed of  one or more simple conditions connected by \nlogical operators (e.g. AND, OR, XOR).\n•E.g., the decision  (x>0 && !y>0) consists of  two conditions,  x>0and !(y>0)\n•The test should cover all the conditions in the program. \n•A condition is covered if  it evaluates to both true and false outcome by at least one test cases.\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nFour condition outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition Coverage =\n\nCondition/Decision Coverage\n17\n•Condition coverage does not imply decision coverage (and vice versa)\n•For condition/decision coverage, the test set should cover all the decisions and \nconditions in the program.\n•Each condition and decision should be evaluated to both true and false outcomes\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\npass= False\nprint(isPass)\nTest CaseexamScoreattendance_rate\nExpected \nResult\n1\n2\nTwo decisions outcomes to be covered:\nexamScore>= 70 and attendance_rate>0.5: {T,F}\nFour outcomes to be covered:\nexamScore>=70: {T,F}, attendance_rate>0.5: {T, F}\nCondition/Decision Coverage =\n\nMultiple condition coverage\n18\n•The coverage domain consists of  all the combinations of  conditions in each \ndecision.\n•A decision with n conditions requires 2\nn\ntest cases!\nTest CaseexamScoreattendance_rateExpected Result\n1\n2\n3\n4\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\n4 combinations of conditions \nTT, TF, FT, FF\nMultiple condition coverage =\n\nModified Condition/Decision (MC/DC) Coverage\n19\nEach simple condition within a compound condition C in P has been shown to independently effect the \noutcome of  C. \nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\nifexamScore>= 70andattendance_rate>0.5:\nisPass= True\nelse\nisPass= False\nprint(isPass)\nThe decision: examScore>= 70 and attendance_rate>0.5\nTo test the independent effect of the first condition, we set the second \ncondition to be True. \nWhy? If we set the second condition to be false, the decision is false no \nmatter what the first condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n1800.7True\n2600.7False\nThe decision: examScore>= 70 and attendance_rate>0.5\nSimilarly, to test the independent effect of the second condition, we \nset the first condition to be True. \nWhy? If we set the first condition to be false, the decision is false no \nmatter what the second condition is True or false.\nTest CaseexamScoreattendance_rateExpected Result\n3800.7True\n4800.4False\n\nPath coverage\n20\n•The coverage domain consists of  all paths in the control \nflow graph (CFG)\n•May generate many test cases if  the CFG is complex \n•e.g. multiple levels of  nested if-else statements, loops\nA\nBC\nD\nEF\nG\nexamScore= int(input(\"Enter exam score: \"))\nattendance_rate= float(input(\"Enter attendance rate: \"))\n# First independent if-else statement\nifexamScore>= 70:\npass_exam= True\nelse:\npass_exam= False\n# Second independent if-else statement\nifattendance_rate> 0.5:\npass_attendance= True\nelse:\npass_attendance= False\nTest CaseexamScoreattendance_rate\n1\n2\n\nLoop Coverage\n•Zero-pass : The loop is not executed at all.\n•Single-pass : The loop is executed exactly once.\n•Two-pass : The loop is executed exactly twice.\n•Multi-pass : The loop is executed more than twice.\n21\n# input number of students\nn = int(input(\"Enter number of students: \"))\nfori inrange(n):\n# Input examScore\nexamScore= int(input(f\"Enterexam score for student {i+1}: \"))\nattendance_rate= float(input(f\"Enterattendance rate for student {i+1}: \"))\n# Print whether the student pass/fail\nifexamScore>= 70andattendance_rate> 0.5:\nisPass= True\nelse:\nisPass= False\nprint(f\"Student{i+1}passed? {isPass}\")\nWhat are the test cases?\n\nComparison of  the coverage criteria based on \ncontrol flow\n22\n•Condition, condition/decision, or decision coverage may not be able to reveal \nsome common faults.\n•Multiple condition coverage may reveal more faults, but it may generate too \nmany test cases.\n•MC/DC coverage is a weaker criterion than the multiple condition coverage \ncriterion. \n•The number of  test cases is much less than multiple condition coverage (in particular when \nthere are many conditions in a decision), but it can detect most types of  faults.\n\nCriteria Subsumption\n23\n•A coverage criterion C1 subsumes C2 if  and only if  every test set that satisfies \ncriterion C1 also satisfies C2.\n•In other words, given a test set which is adequate with respect to C1, it is also adequate \nwith respect to C2.\n•For example,\n•Decision coverage subsumes statement coverage.\n•If  a test set traverses both the true/false outcome of  every decisions in a CFG, all the \nstatements are also traversed.\nDecision \nCoverage\nStatement \ncoverage\n\nHierarchy of  coverage criteria\n24\nPath Coverage\nDecision CoverageCondition Coverage\nStatement Coverage\nDecision/Condition \nCoverage\nMultiple condition \nCoverage\nMC/DC \nCoverage\n\nState transition testing\n25\n•In many systems, behaviour/output depends on the input as well as the current state\n•The current state depends on \n•The initial state \n•The sequence of  inputs the system has received in the past\n•Example: ON/OFF button,  ATM \n\nState transition diagram\n26\n•In state transition testing, test cases are generated from the state-transition diagrams (the \nspecification).\n•The diagram documents the states that a system can exist in and the events that come into \nand are processed by a system as well as the system's responses\n•Components\n•State\n•Condition in which a system is waiting for one or more events. \n•Represented by a circle\n•Event\n•May be external (e.g. input by the user) or event generated within the system (e.g. timeout event). \n•With an event, the system can change state or remain in the same state and/or execute an action\n•Action\n•An operation initiated because of  a state change\n•E.g. output a certain message, start a timer, generate a ticket\n\nNotation\n27\n•Transition (represented by an arrow)\n•represents a change from one state to another\n•each transition can be associated with an event (which triggers the transition) and action (which \nspecify the behaviour/output)\n•Entry point (represented as a black dot)\n•Exit point (represented as a bulls-eye symbol)\nS1S2\nEvent [condition] / action\n\nExample: ON/OFF Button for remote \ncontrol\n28\nONOFF\nPress on / TV on\nPress OFF / TV off\nEntry point\nExit point\nState\nTest case:\n1) (Press ON, Press OFF)\nEvent\nAction\nTransition\n\nCoverage Criteria for state transition \ntesting\n29\nstates\n \nof\n \n.\n \ntotal\nexercised\n \nstates\n \nof\n \n.\n \n \ncoverage\n \nstate\nno\nno\n=\nns\n transitio\nof\n \n.\n \ntotal\nexercised\n \nns\n transitio\nof\n \n.\n \n \ncoverage\n \ntransition\nno\nno\n=\nstransition-2 of sequences of . total\nexercised ns transitio2 of sequences of .\n  coverage transition-2\nno\nno\n=\ninputs of .  states of .\nexercised pairsinput -state of .\n  coverageinput -state\nnono\nno\n\n=\nNote:  We can generalize the 2-transition coverage to n transitions coverage.\n\nImproved test set\n30\nONOFF\nPress on / TV on\nPress OFF / TV off\nTest case:\n1)(Press ON, Press OFF, Press ON, Press OFF)\n2)(Press OFF, Press OFF, Press ON, Press ON, Press OFF)\n\nExample: Airline reservation application\n31\nThe customer first provide some information including departure and \ndestination cities, dates, and times, which is used by the reservation agent to \nmake a reservation. \nThe reservation is now at the “made”state.\n\n32\n•If  the customer pays before the startPayTimerexpires, the reservation \ntransits to the “Paid”state. \n\n33\n•From the Paid state the Reservation transitions to the Ticketed state when the print \ncommand (an event) is issued. \n•In addition to entering the Ticketed state, a Ticket is output by the system.\n\n34\n•From the Ticketed state we giveTicketto the gate agent to board the \nplane.\n\nSome alternative paths\n35\n•If  the reservation is not paid for in the time allotted (the PayTimerexpires), it is \ncancelled for non-payment.\n•From the Made state the customer (through the reservation agent) asks to cancel the \nreservation. A new state, Cancelled By Customer, is required.\n•In addition, a reservation can be cancelled from the Paid state. In this case a Refund \nshould be generated and leave the system. The resulting state again is Cancelled By \nCustomer. \n•From the Ticketed state the customer can cancel the Reservation. The airline will \ngenerate a refund but only when it receives the printed ticket from the customer.\n\nExercise\n36\nPerform state transition testing for the airline reservation application.\nWhat is the minimum number of  test cases required to \ni)Covers all states?\nii)Covers all transitions?\n\nTest Automation\n39\n\nAutomated testing and CI at Google\n40\nAutomated\nTests\n\nTest Execution TimeLimit/Resource Usage for Google \nSoftware Development\n41\n•In software testing,a mock is a simulated \nobject that mimics the behavior of a real object \nin controlled ways. \n•It is used to test the behavior of other objects \nin the system under test (SUT) that depend on \nthe real object\n\nTest SizeDefinitions \n•Small tests\n•Verify the behavior of  a single unit of  code \ngenerally isolated from its environment. \n•Examples\n•a single class or a small group of  related functions. \nSmall tests should have no external dependencies\n42\n\n•Medium testsvalidate the interaction of  \none or more application modules\n•Aimed at testing interaction across a limited \nsubset of  modules\n•Often do not execute as frequently as smaller \ntests.\n43\n\n•Largeandenormous tests\n•“system tests” or “end-to-end tests”\n•Exercise any or all application subsystems from the UI down to \nbackend data storage\n•make use of  external resources such as databases, file systems, and \nnetwork services.\n44\nExample of end-to-end test\n•Anew user signs up on the platform.\n•After registration, the user logs in to the system.\n•The user then selects a book and adds it to the shopping cart.\n•The user checkout by providing shipping details and choosing a shipping option.\n•The user proceeds to \"Payment,\" where he/she enter payment information and submit it.\n•After the payment is processed, the user receives an \"Order Confirmation.\"\n\nOverview of  Test automation\n45\n•Test automation involves the use of  software to control the execution of  tests\n•Agile developers emphasize the importance of  automatedtests. \n•With short cycles, manual regression testing is nearly impossible.\n•Layers of  test automation\n•Unit Test\n•Service-Layer testing\n•UI testing\nSlow\nFast\n$$$\n$\n\nWriting automated tests\n•Hand-Scripted Tests\n•hand-coding of  test programs (\"scripts\") that exercise the system.\n•E.g. xUnit\n•Recorded Test \n•The use of  tools that monitor our interactions (mostly at UI level) with the SUT while we test it manually.\n•Test cases are saved to and becomes the script for replaying this test against another (or even the same) version of  the SUT.\n•Use of  GenAI(e.g. GitHub Copilot) \n\nCode Driven Testing/Unit Tests\n47\n•Verify functionality of  a small subset of  the system, such as an object or \nmethod. \n\nJUnit\n•JUnit is a popular unit testing framework in the Java ecosystem\n•Unit testing involves testing the smallest parts of  an application, like methods and classes, in \nisolation from the rest of  the system to ensure they perform as expected.\n•Help developers write and run repeatable automated tests to improve software quality.\n•JUnit 5 being the latest version\n\nAssertions\n•In JUnit, the input and expected behavior of  test cases are specified using assertions.\n•Here are some examples of  assertions\n•assertEquals(int expected, int actual): This assertion passes if  expected and actual are equal \naccording to the == operator; otherwise, the assertion fails. \n•For each primitive type int, float, double, char, byte, long, short, and boolean, the \nassertion has an overloaded version.\n•assertEquals(double expected, double actual, double tolerance): This assertion passes if  the absolute \nvalue of  the difference between expected and actual is less than or equal to the tolerance \nvalue; otherwise, the assertion fails. The assertion has an overloaded version for float inputs.\n•assertSame(Object expected, Object actual): This assertion passes if  the expected and actual values \nrefer to the same object in memory; otherwise, the assertion fails.\n•assertTrue(Boolean condition): This assertion passes if  the condition is true; otherwise, it fails.\n49\n\nJUnit example\n//incorrect code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturn b -a;\n}\n}\nJUnitTests\nTested by\n50\nTest \nResults\n•Each JUnit test method is annotated with @Test. \n•JUnit will execute methods annotated with @Test during the test run.\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\nSystem under test (SUT)\n\n//correct code\npublicclassCalculation{\npublicstaticintadd(inta, intb) {\n//compute the sum of a and b\nreturna + b;\n}\npublicstaticintsub(inta, intb) {\n//Subtract b from a\nreturna -b\n}\n}\nJUnitTests\nTested by\n51\nTest \nResults\nimportstaticorg.junit.Assert.*;\nimportorg.junit.Test;\npublicclassCalculationTest{\n@Test\npublicvoidtestAdd(){\nintexpected= 8;\nintactual= Calculation.add(5,3);\nassertEquals(actual, expected);\n}\n@Test\npublicvoidtestSubtract(){\nintexpected= 2;\nintactual= Calculation.sub(5,3);\nassertEquals(actual, expected);\n}\n}\n\nExample: Testing of  Shopping Cart \n52\nclassShoppingCartTest{\n@Test\nvoidaddMultipleItemsToCart() {\nShoppingCartcart= newShoppingCart();\ncart.addItem(2, 19.99);\ncart.addItem(1, 9.99);\nassertEquals(3, cart.getItemsCount(), \n\"Cart should have 3 items after adding.\");\n// Testing floating-point numbers\nassertEquals(49.97, cart.getTotalAmount(), 0.001, \n\"Total amount should be 49.97\");\n}\n}\n$19.99 each\n$9.99 each\n\nIntegration Layer Tests\n•Test the underlying services of  an application \n\nIntegration layer tests for web applications\n•Hypertext Transfer Protocol (HTTP)\n•the protocol the web speaks to send and receive information from one \nplace to another.\nGET /login.html\nHost: abc.com\nHTTP/1.1 200 OK\nContent-Type: text/html\n<!DOCTYPE html>\n<html>\n<head>\n<title>Please login</title>\n</head>\n<body>\n<h1>Please sign in</h1>\n....\n</body>\n</html>\n\nAutomated Testing of  REST API\n•Send HTTP request to drive tests\n•Verify the HTTP response with assertions\n\n56\nGET /repos/{owner}/{repo}/issues/42 HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nHTTP/1.1 200 OK\nServer: GitHub.com\nDate: Sun, 23 Feb 2024 12:00:00 GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 200 OK\n{\n\"id\": 123456789,\n\"number\": 42,\n\"title\": \"Example Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-20T10:00:00Z\",\n\"updated_at\": \"2024-02-21T15:30:00Z\",\n\"closed_at\": null,\n\"body\": \"This is an example issue.\",\n\"user\": {\n\"login\": \"...\"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"...\",\n\"full_name\": “...\",\n}\n}\n@test\nvoidshouldGetIssue(){\nIssueissue= gitHub.getIssue(\"42\");\nassertEquals(\"Example Issue\", issue.getTitle());\nassertEquals(\"open\", issue.getState());\nassertEquals(\"This is an example issue.\", issue.getBody());\n}\nREST API\n\n57\nPOST /repos/{owner}/{repo}/issues HTTP/1.1\nHost: api.github.com\nUser-Agent: Your-User-Agent\nAuthorization: token Your-Access-Token\nAccept: application/vnd.github.v3+json\nContent-Type: application/json\nContent-Length: XXX\n{\n\"title\": \"New Issue\",\n\"body\": \"This is a new issue.\"\n}\nHTTP/1.1201Created\nServer: GitHub.com\nDate: Sun, 23Feb 202412:00:00GMT\nContent-Type: application/vnd.github.v3+json\nContent-Length: XXX\nConnection: keep-alive\nStatus: 201Created\n...\n{\n\"id\": 123456790,\n\"number\": 43,\n\"title\": \"New Issue\",\n\"state\": \"open\",\n\"created_at\": \"2024-02-23T12:00\n\"updated_at\": \"2024-02-23T12:00\n\"closed_at\":null,\n\"body\": \"This is a new issue.\",\n\"user\": {\n\"login\": \"example_user\",\n\"avatar_url\": \"\n\"html_url\": \"\n},\n\"repository\": {\n\"id\": 987654321,\n\"name\": \"example-repo\",\n\"full_name\": \"example_user/example-repo\",\n\"html_url\": \"\n}\n}\n@test\nvoidshouldCreateIssue(){\nIssueissue= newIssue();\nissue.setTitle(\"New Issue\");\nissue.setBody(\"This is a new issue.\");\nIssuenewIssue= gitHub.createIssue(issue);\nassertEquals(\"New Issue\", newIssue.getTitle());\nassertEquals(\"This is a new issue.\", newIssue.getBody());\n}\nREST API\n\nIntegration Layer Tests: Pros and Cons\n•Not having to deal with the fragility of  the UI\n•Not most precise\n•Telling you something is broken, they can’t always tell you exactly where.\n\nUser Interface (UI) Tests\n•Generates user interface events (e.g. keystrokes, mouse clicks), and observes the \nchanges that result in the user interface\n•Test the application from the UI layer down.\n\nSelenium\n60\n•Selenium is a popular framework for testing web applications\n•Provides a rich set of  commands for fully testing your web-app\n•Test the existence of  UI elements based on their HTML tags\n•Test for specific content, test for broken links, input fields, selection list options, submitting forms, and table \ndata\n•Testing of  window size, mouse position, alerts, Ajax functionality, pop up windows, event handling,  etc.\nhttps://www.selenium.dev\n\nInstalling Selenium Chrome Extension\n61\nhttps://chrome.google.com/webstore/detail/selenium-ide/mooikfkahbdckldjjndioackbalphokd?hl=en\n\nExample\n62\nExport as JUnit Tests\n\nUser Interface Test: Advantages\n•Automation is always feasible \n•End-to-end\n•exercising all the different parts of  the application\n•the user interface, the underlying services, all the way to the database. \n•Often used for high-level smoke tests.\n•super high-level tests that verify that at some basic level our system is up and running\n•Check that the applications are correctly deployed, correctly configured, all the pieces of  our \narchitecture are connected and hooked up right\n\nUser Interface Test: Limitations\n•Slow\n•orders of  magnitude slower than unit tests.\n•UI test may be fragile\n•broken when there are changes in UI\n•Not very precise\n\nAdvantages of  automated testing\n65\n•Fast\n•Automated Tools run tests significantly faster \nthan human users\n•Good for load testing, massive random testing,  \netc\n•Reliable\n•Tests perform precisely the same operations \neach time they are run, thereby eliminating \nhuman error \n•Repeatable\n•You can test how the software reacts under \nrepeated execution of  the same operations. \n\nClimbing the Pyramid\n•Start with Unit Test\n•Step Up to the Integration Tests\n•Reach for the UI Tests\n\nStart with Unit test\n•Most teams start with unit tests because unit tests are what developers write \nevery time they add a feature to the system.\n•The tests are cheap, so we don’t have to do as much later near the top.\n“Test as much of this as you reasonably can, but \nunderstand that you won’t get it all.”\n\nStep Up to the Integration Tests\n•Looking for gaps and high-level connectivity.\n•Do the web requests flow down to the database? \n•Is the authentication service correctly connected to the login code?\n•In web applications, integration test may focus onthe testing of  our web services, \nwhile unit testing will be the testing of  the underlying objects.\n\nReach for the UI Tests\n•Check for end-to-end system confirmation and connectivity with the \nUI.\n•Push as much testing as you possibly can further down the pyramid\n•the tests are faster, more reliable, and less flaky.\n\nPerformance Testing\n•LoadTesting\n•Stress Testing\n•SoakTesting\n70\n\nLoad Testing\n71\n•Performance tests under anticipated production load (normal and peak load conditions)\n•Objectives\n•To determine the response times for various time critical transactions and business processes \n•Helps to identify the maximum operating capacity of  an application as well as any bottlenecks that \nmight interfere with its operating at capacity \n•Ensure that the response times are within documented expectations \n•e.g. Service Level Agreements -SLAs\n\nStress testing\n72\n•Determine or validate an application’s behaviour when it is pushed beyond \nnormal or peak load condition.\n•To know in advance if  a ‘stress’ situation will result in a catastrophic system \nfailure, or if  everything just “goes really slow”\n\nStress scenarios\n73\nType of ApplicationCircumstances that could give rise to Stress levels of activity.\nOnline Banking\nAfter an outage -when many clients have been waiting for access \nto the application to do their banking transactions.\nMarketing / Sales Application\nVery successful advertising campaign -or substantial error in \nadvertising campaign that understates pricing details.\nVarious applications \nUnexpected publicity -for example, in a news article in a national \nonline newspaper.\n\nSoak tests (aka Endurance Testing)\n74\n•It is possible that a system may ‘stop’ working after a certain number of  \ntransactions have been processed\n•E.g. Due to failure to release resources (e.g. memory) properly\n•Soak test involves running a system at high levels of  load for prolonged periods of  \ntime\n•Weekends are often a good time for a soak test.\n\nK6\n75\n•A performance  testing tool\n•Automated Testing of  websites/RESTful APIs\nhttps://k6.io\n\nHTTP-specific built-in metrics\n76\nMETRIC NAMEDESCRIPTION\nhttp_reqsHow many HTTP requests has k6 generated, in total.\nhttp_req_blocked\nTime spent blocked (waiting for a free TCP connection slot) before initiating the \nrequest.float\nhttp_req_connectingTime spent establishing TCP connection to the remote host.float\nhttp_req_tls_handshakingTime spent handshaking TLS session with remote host\nhttp_req_sendingTime spent sending data to the remote host.\nhttp_req_waiting\nTime spent waiting for response from remote host (a.k.a. “time to first byte”, or \n“TTFB”).\nhttp_req_receivingTime spent receiving response data from the remote host.\nhttp_req_duration\nTotal time for the request. (i.e. how long did the remote server take to process the \nrequest and respond, without the initial DNS lookup/connection times).\nhttp_req_failedThe rate of failed requests.\nhttp_req_duration= http_req_sending+ http_req_waiting+ http_req_receiving\n\nK6 Cloud\n77\nhttps://k6.io/cloud\n\nContinuous Integration\n78\n\nMerging and integration\n•Merging is much easier to do frequently and \nsmall rather than rarely and large\n•Less code changes that might hold up conflicts\n•Smaller integrations mean less work and \nreduced risk\n79\n\nLow vs. High frequency Integration\n80\nhttps://martinfowler.com/articles/branching-patterns.html\nHigh-Frequency IntegrationLow-Frequency Integration\nIntegration PaceFrequent, multiple times a dayInfrequent, possibly daily or weekly\nFeedback Loop\nShort, immediate feedback on \nchanges\nLonger, feedback delayed until \nintegration\nMerge Complexity\nTypically lower due to smaller \nchanges\nHigher, as changes accumulate\nRisk of Conflicts\nLower, conflicts detected and \nresolved quickly\nHigher, conflicts may be more \ncomplex to solve\n\nWhat Is Continuous Integration (CI)? \n•A software development practice where members of  a team integrate their work frequently\n•Usually each person integrates at least daily\n•Each integration is verified by an automated build (including test) to detect integration errors as quickly as \npossible\n•Significantly reduced integration problems and allows a team to develop cohesive software more rapidly.\n\nBasic CI Lifecycle\nhttps://code-maze.com/what-is-continuous-integration\n\nCI Practices\nPracticeDescriptionRationaleBest Practices & Tips\nCommit code frequently\nIntegrate changes into the main \nbranch often, at least daily.\nHelps to reduce integration \nconflicts and allows for early \ndetection of issues.\nEnsure commits are small and \nmanageable for easier \ntroubleshooting.\nWrite unit tests\nCreate automated tests that \ncover individual units of the \ncodebase to ensure each part \nfunctions as expected.\nEnsures that new code does not \nbreak existing functionality and \nhelps in maintaining code quality.\nAim for a high level of code \ncoverage and run tests before \ncommitting changes.\nAll tests must pass\nBefore merging, all unit tests and \nintegration tests should pass.\nConfirms that the code adheres \nto the expected behavior and \nreduces the chance of bugs.\nIntegrate a testing framework \nthat runs tests automatically on \neach commit.\nFix broken builds immediately\nPrioritize fixing a broken build to \nensure the main branch is always \nin a deployable state.\nMinimizes downtime and keeps \nthe codebase stable for all \ndevelopers.\nImplement monitoring and alerts \nfor broken builds to address them \npromptly.\n\nContinuous Integration Servers\n•Software tool that centralizes all your CI operations and provides a reliable and \nstable environment for you to build your projects on\n\nFeatures of  CI Servers\n•Monitors your project’s repository. On commit to certain branches, it pulls the \nchanges and latest version of  your code from the code repository\n•Performs the tasks you defined\n•Running the build scripts, automated tests\n•Upon completionof  the tasks, CI server sends feedback to the relevant project \nmembers with the details of  the build.\n•Other features\n•Code analysis, code coverage, code quality reports, etc\n85\n\nGitHub Action\n•Automate your software workflows and CI/CD. \n•Build, test, and deploy your code right from GitHub\n•Overview\n•https://docs.github.com/en/actions\n86\nhttps://github.com/microsoft/vscode\n\nExample: Creating CI pipeline\n87\nhttps://github.com/cswclui/github_action\n\n88\n\n89\n\n90\n\n91\n\n92\nIntroduce a bug in the program and commit the change\n\nExample: CICD in a Java Maven Project\n93\nhttps://github.com/polyurichard/JavaCICD",
      "flashcards": [
        {
          "question": "What is GPTutor?",
          "answer": "An AI-powered educational tool that transforms PDFs into study materials."
        },
        {
          "question": "What formats does GPTutor support?",
          "answer": "Flashcards, summaries, and Cornell notes."
        },
        {
          "question": "How does GPTutor work?",
          "answer": "It extracts text from PDFs and uses AI to convert it into various study formats."
        },
        {
          "question": "What APIs does GPTutor use?",
          "answer": "It can use either OpenAI or GitHub APIs to generate educational content."
        },
        {
          "question": "What is the benefit of using GPTutor?",
          "answer": "It helps students learn more effectively by transforming documents into interactive study materials."
        }
      ],
      "summary": "GPTutor is an innovative educational application designed to enhance the learning experience by transforming PDF documents into various study formats. The app uses advanced AI technology to extract text from uploaded PDFs and convert it into flashcards for active recall, summaries for quick review, and Cornell notes for structured learning. GPTutor supports multiple AI backends including OpenAI and GitHub APIs, giving users flexibility in how their content is processed. The application is built with a modern web stack and provides an intuitive interface for uploading documents and selecting output formats. Even without a database connection, GPTutor maintains functionality by storing results in memory, ensuring users can still benefit from its features.",
      "cornellNotes": {
        "cues": [
          "GPTutor Purpose",
          "Study Formats",
          "Technology Stack",
          "AI Integration"
        ],
        "notes": [
          "GPTutor helps students learn by transforming documents into study materials",
          "Supports flashcards for active recall, summaries for overview, and Cornell notes for structured learning",
          "Built with MERN stack (MongoDB, Express, React, Node.js) with fallback mechanisms for database unavailability",
          "Integrates with multiple AI providers including OpenAI and GitHub API"
        ],
        "summary": "GPTutor is an AI-powered educational tool that transforms PDF documents into various study formats to enhance learning efficiency, with support for multiple AI backends and robust error handling."
      },
      "multipleChoice": [
        {
          "question": "What is the primary purpose of GPTutor?",
          "options": [
            "Creating presentations",
            "Transforming PDFs into study materials",
            "Writing essays",
            "Taking notes"
          ],
          "correctAnswer": 1
        },
        {
          "question": "Which of these formats is NOT supported by GPTutor?",
          "options": [
            "Flashcards",
            "Cornell Notes",
            "Mind Maps",
            "Summaries"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What AI technology does GPTutor integrate with?",
          "options": [
            "Only OpenAI",
            "Only GitHub API",
            "Both OpenAI and GitHub API",
            "Neither OpenAI nor GitHub API"
          ],
          "correctAnswer": 2
        }
      ],
      "createdAt": "2025-04-01T10:37:18.771Z",
      "isMockData": true
    },
    {
      "id": "1743503888267",
      "fileName": "receipt_ISD Asm1.pdf.pdf",
      "originalText": "\n\nSubmission author:\nAssignment title:\nSubmission title:\nFile name:\nFile size:\nPage count:\nWord count:\nCharacter count:\nSubmission date:\nSubmission ID:\nDigital Receipt\nThis receipt acknowledges that Turnitin received your paper. Below you will find the receipt\ninformation regarding your submission.\nThe first page of your submissions is displayed below.\nWing Cheuk YUE\nAssignment submission\nISD Asm1.pdf\nISD_Asm1.pdf\n3.89M\n20\n1,079\n5,313\n07-Mar-2025 07:47PM (UTC+0800)\n2607892964\nCopyright 2025 Turnitin. All rights reserved.",
      "flashcards": [
        {
          "question": "What is the purpose of the digital receipt provided by Turnitin?",
          "answer": "The digital receipt acknowledges that Turnitin has received your paper and provides submission details."
        },
        {
          "question": "Who is the author of the submission in the provided text?",
          "answer": "Wing Cheuk YUE"
        },
        {
          "question": "What is the title of the assignment submitted?",
          "answer": "Assignment submission"
        },
        {
          "question": "What is the file size of the submission?",
          "answer": "3.89M"
        },
        {
          "question": "How many pages are in the submitted document?",
          "answer": "20"
        },
        {
          "question": "What is the word count of the submitted paper?",
          "answer": "1,079"
        },
        {
          "question": "What is the character count of the submission?",
          "answer": "5,313"
        },
        {
          "question": "When was the submission made?",
          "answer": "07-Mar-2025 at 07:47 PM (UTC+0800)"
        },
        {
          "question": "What is the submission ID for the paper?",
          "answer": "2607892964"
        }
      ],
      "summary": "The provided text is a digital receipt from Turnitin, confirming the submission of an assignment by Wing Cheuk YUE. The receipt includes essential details about the submission, which are crucial for both the author and any academic institution involved in the evaluation process.\n\nKey information presented in the receipt includes the author's name (Wing Cheuk YUE), the title of the assignment (\"Assignment submission\"), and the file name (\"ISD Asm1.pdf\"). The document size is noted as 3.89 MB, and it consists of 20 pages. The word count is 1,079, and the character count is 5,313, providing a clear understanding of the length and scope of the submitted work. \n\nThe submission was made on March 7, 2025, at 07:47 PM (UTC+0800), which indicates the specific timeframe in which the author completed the submission. Additionally, a unique Submission ID (2607892964) is included, serving as a reference for tracking and identifying the submission within Turnitin’s system.\n\nOverall, this digital receipt serves as an official acknowledgment from Turnitin that the author’s paper has been successfully received and is now part of their database for plagiarism checking and academic integrity verification. The receipt also emphasizes Turnitin's copyright notice for the year 2025, ensuring that users are aware of the proprietary nature of the service.",
      "cornellNotes": {
        "cues": [
          "Submission author",
          "Assignment title",
          "Submission title",
          "File information",
          "Submission details",
          "Digital Receipt"
        ],
        "notes": [
          "Wing Cheuk YUE",
          "Assignment submission",
          "ISD Asm1.pdf",
          "File name: ISD_Asm1.pdf",
          "File size: 3.89M",
          "Page count: 20",
          "Word count: 1,079",
          "Character count: 5,313",
          "Submission date: 07-Mar-2025 07:47PM (UTC+0800)",
          "Submission ID: 2607892964",
          "Acknowledgment of receipt of paper by Turnitin"
        ],
        "summary": "This document contains the details of a paper submission to Turnitin, including the author, title, file specifications, and submission metadata along with a digital receipt confirming the submission."
      },
      "multipleChoice": [
        {
          "question": "Who is the author of the submission?",
          "options": [
            "Wing Cheuk YUE",
            "Turnitin",
            "ISD Asm1.pdf",
            "Digital Receipt"
          ],
          "correctAnswer": 0
        },
        {
          "question": "What is the file size of the submitted assignment?",
          "options": [
            "1.07M",
            "3.89M",
            "5,313",
            "20"
          ],
          "correctAnswer": 1
        },
        {
          "question": "What is the character count of the submission?",
          "options": [
            "1,079",
            "3.89M",
            "5,313",
            "20"
          ],
          "correctAnswer": 2
        },
        {
          "question": "What date was the assignment submitted?",
          "options": [
            "07-Mar-2025",
            "08-Mar-2025",
            "06-Mar-2025",
            "05-Mar-2025"
          ],
          "correctAnswer": 0
        },
        {
          "question": "What is the Submission ID for the submitted paper?",
          "options": [
            "2607892964",
            "ISD Asm1.pdf",
            "3.89M",
            "07-Mar-2025"
          ],
          "correctAnswer": 0
        }
      ],
      "createdAt": "2025-04-01T10:38:08.267Z",
      "isMockData": false
    }
  ]
}